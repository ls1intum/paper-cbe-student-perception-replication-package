{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "8c0e964e4f2bf215",
            "metadata": {},
            "source": [
                "Analysis Pipeline for Evaluation Data\n",
                "\n",
                "This notebook documents the pipeline used to analyze the evaluation data for the scientific paper *Investigating Student Interaction with Competency-Based CS Education*. The analysis focuses on extracting insights into the effectiveness and impact of *Atlas* on student's learning process.\n",
                "\n",
                "## Objectives\n",
                "- **Data Cleaning and Preprocessing:** Standardize and clean the raw data collected from the Learning Management System (LMS) and surveys.\n",
                "- **Descriptive Statistics:** Summarize key metrics, such as student engagement, performance, and competency mastery.\n",
                "- **Inferential Analysis:** Apply statistical tests to evaluate the significance of observed outcomes.\n",
                "- **Visualization:** Generate clear and insightful plots to highlight trends and findings.\n",
                "\n",
                "## Tools and Libraries\n",
                "This analysis leverages Python's robust data science ecosystem, including:\n",
                "- `pandas` for data loading, cleaning, and reshaping\n",
                "- `matplotlib` for flexible plotting and export\n",
                "- `plot_likert` for clear Likert-scale visualizations\n",
                "- `pingouin` for inferential statistics and effect sizes\n",
                "- `numpy` for fast vectorized numerical operations\n",
                "- `os` for reliable file and path handling\n",
                "\n",
                "Each section of this notebook corresponds to a specific stage of the analysis pipeline, ensuring clarity and reproducibility."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70af508c6631f323",
            "metadata": {},
            "source": [
                "# Imports\n",
                "In the first step, we import all necessary libraries and load the datasets required for the analysis."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3bd3ce5739813500",
            "metadata": {},
            "source": [
                "## Libraries and Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "469685661c3c2096",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:53.894629Z",
                    "start_time": "2025-09-27T16:58:53.884588Z"
                }
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import plot_likert as pl\n",
                "import pingouin as pg\n",
                "import os\n",
                "import numpy as np\n",
                "from statsmodels.stats.proportion import proportions_ztest\n",
                "from statsmodels.stats.multitest import multipletests\n",
                "from statsmodels.stats.contingency_tables import Table2x2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9227e80707c70f4e",
            "metadata": {},
            "source": [
                "## Data"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7df8010370e282d3",
            "metadata": {},
            "source": [
                "### System Data\n",
                "Load all relevant system log datasets needed for the data exploration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56b203c4e823a726",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:55.930259Z",
                    "start_time": "2025-09-27T16:58:53.903580Z"
                }
            },
            "outputs": [],
            "source": [
                "competency_df = pd.read_csv('data/01_anonymized/competency.csv')\n",
                "competency_exercise_mapping_df = pd.read_csv('data/01_anonymized/competency_exercise.csv')\n",
                "competency_unit_mapping_df = pd.read_csv('data/01_anonymized/competency_lecture_unit.csv')\n",
                "competency_user_df = pd.read_csv('data/01_anonymized/competency_user.csv')\n",
                "exam_score_df = pd.read_csv('data/01_anonymized/participant_score_exam.csv')\n",
                "exercise_df = pd.read_csv('data/01_anonymized/exercise.csv')\n",
                "learning_path_df = pd.read_csv('data/01_anonymized/learning_path.csv')\n",
                "lecture_df = pd.read_csv('data/01_anonymized/lecture.csv')\n",
                "participation_df = pd.read_csv('data/01_anonymized/participation.csv')\n",
                "participant_score_df = pd.read_csv('data/01_anonymized/participant_score.csv')\n",
                "science_event_df = pd.read_csv('data/01_anonymized/science_event.csv')\n",
                "units_df = pd.read_csv('data/01_anonymized/lecture_unit.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "84082f67a1e87acb",
            "metadata": {},
            "source": [
                "### Survey Data\n",
                "Load the pre- and post-survey datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee10369766c3794a",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.032257Z",
                    "start_time": "2025-09-27T16:58:56.027719Z"
                }
            },
            "outputs": [],
            "source": [
                "pre_survey_df = pd.read_csv('data/01_anonymized/lime_survey_pre.csv')\n",
                "post_survey_df = pd.read_csv('data/01_anonymized/lime_survey_post.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dbc5c748157bd4d",
            "metadata": {},
            "source": [
                "# Data Cleaning and Preprocessing\n",
                "After the data is loaded, it is crucial to preprocess it to ensure consistency and relevance for the analysis."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f1adb814e9933a1f",
            "metadata": {},
            "source": [
                "## Remove all Science Events that are not related to the relevant course\n",
                "The science_event_df contains events from multiple courses. We filter it to retain only those events that are relevant to our specific course of interest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c76dc052f8040adf",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.302969Z",
                    "start_time": "2025-09-27T16:58:56.083249Z"
                }
            },
            "outputs": [],
            "source": [
                "# Define all relevant resources\n",
                "# Course\n",
                "course_id = 443\n",
                "# Exercises\n",
                "exercise_ids = exercise_df['id'].dropna().unique()\n",
                "# Competencies\n",
                "competency_ids = competency_df['id'].dropna().unique()\n",
                "# Units\n",
                "unit_ids = units_df['id'].dropna().unique()\n",
                "\n",
                "## Filter science_event_df for relevant events\n",
                "science_event_df = science_event_df[\n",
                "    (\n",
                "        ((science_event_df['event_type'] == 1) & science_event_df['resource_id'].isin(unit_ids))\n",
                "        |\n",
                "        ((science_event_df['event_type'] == 2) & science_event_df['resource_id'].isin(exercise_ids))\n",
                "        |\n",
                "        ((science_event_df['event_type'] == 3) & science_event_df['resource_id'].isin(competency_ids))\n",
                "        |\n",
                "        ((science_event_df['event_type'] == 4) & (science_event_df['resource_id'] == course_id))\n",
                "        |\n",
                "        (science_event_df['event_type'].isin([5, 6, 7, 8, 9]))\n",
                "    )\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "80ba56a40f3dcb07",
            "metadata": {},
            "source": [
                "## Rename Columns\n",
                "For consistency across datasets, we rename certain columns to have uniform naming conventions, especially for user identifiers. For better clarity, we also rename the analyzed survey question columns to more descriptive names."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c4276fd03a4c732",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.362936Z",
                    "start_time": "2025-09-27T16:58:56.343397Z"
                }
            },
            "outputs": [],
            "source": [
                "participation_df = participation_df.rename(columns={'student_id': 'user_id'})\n",
                "science_event_df = science_event_df.rename(columns={'identity': 'user_id'})\n",
                "pre_survey_df = pre_survey_df.rename(columns={\n",
                "        'uuid': 'user_id',\n",
                "        'G01Q08': 'Degree',\n",
                "        'G03Q07': 'Subject',\n",
                "        'G02Q09': 'Semester',\n",
                "        'G03Q10': 'Age',\n",
                "        'G03Q11': 'Gender',\n",
                "    })\n",
                "post_survey_df = post_survey_df.rename(\n",
                "    columns={\n",
                "        'uuid': 'user_id',\n",
                "        'G02Q05': 'Usability',\n",
                "        'G02Q06': 'Transparency',\n",
                "        'G03Q13': 'Confidence',\n",
                "        'G03Q12[SQ005]': 'Frustration',\n",
                "        'G04Q15': 'Participation',\n",
                "        'G05Q16': 'Degree',\n",
                "        'G05Q17': 'Subject',\n",
                "        'G05Q18': 'Semester',\n",
                "        'G05Q19': 'Age',\n",
                "        'G05Q20': 'Gender',\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e552ae8cbc7332a",
            "metadata": {},
            "source": [
                "## Rename Answers\n",
                "The survey responses have answer codes. For better readability, these are mapped to the answers in natural language."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "252cda99aa488022",
            "metadata": {},
            "source": [
                "### Create Answercode mappings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a8b463ace168ba19",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.416246Z",
                    "start_time": "2025-09-27T16:58:56.412072Z"
                }
            },
            "outputs": [],
            "source": [
                "# Answercode mappings\n",
                "participation_mapping = {\n",
                "    'AO01': 'Yes',\n",
                "    'AO02': 'No',\n",
                "}\n",
                "\n",
                "degree_mapping = {\n",
                "    'AO01': 'Bachelor',\n",
                "    'AO02': 'Master',\n",
                "}\n",
                "subject_mapping = {\n",
                "    'AO01': 'Computer Science',\n",
                "    'AO02': 'Information Systems',\n",
                "    'AO03': 'Games Engineering',\n",
                "    'AO04': 'Management and Technology',\n",
                "    'AO05': 'Other',\n",
                "    'AO06': 'Management and Technology',\n",
                "}\n",
                "semester_mapping = {\n",
                "    'AO01': '1',\n",
                "    'AO02': '2',\n",
                "    'AO03': '3',\n",
                "    'AO04': '4',\n",
                "    'AO05': '5',\n",
                "    'AO06': '6',\n",
                "    'AO07': '7',\n",
                "    'AO08': '8',\n",
                "    'AO09': '9+',\n",
                "}\n",
                "age_mapping = {\n",
                "    'AO01': '18',\n",
                "    'AO02': '19',\n",
                "    'AO03': '20',\n",
                "    'AO04': '21',\n",
                "    'AO05': '22',\n",
                "    'AO06': '23',\n",
                "    'AO07': '24',\n",
                "    'AO08': '25',\n",
                "}\n",
                "gender_mapping = {\n",
                "    'AO01': 'Female',\n",
                "    'AO02': 'Male',\n",
                "    'AO03': 'Other',\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "574333f39605704b",
            "metadata": {},
            "source": [
                "### Apply Mapping to Pre-Survey"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "480899c976b6f8e8",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.464249Z",
                    "start_time": "2025-09-27T16:58:56.461037Z"
                }
            },
            "outputs": [],
            "source": [
                "# Apply mappings\n",
                "pre_survey_df['Degree'] = pre_survey_df['Degree'].map(lambda x: degree_mapping.get(x, x))\n",
                "pre_survey_df['Subject'] = pre_survey_df['Subject'].map(lambda x: subject_mapping.get(x, x))\n",
                "pre_survey_df['Semester'] = pre_survey_df['Semester'].map(lambda x: semester_mapping.get(x, x))\n",
                "pre_survey_df['Age'] = pre_survey_df['Age'].map(lambda x: age_mapping.get(x, x))\n",
                "pre_survey_df['Gender'] = pre_survey_df['Gender'].map(lambda x: gender_mapping.get(x, x))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "64d8619716845dd6",
            "metadata": {},
            "source": [
                "### Apply Mapping to Post-Survey"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d9fb04d9b5f1fca",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.520221Z",
                    "start_time": "2025-09-27T16:58:56.516886Z"
                }
            },
            "outputs": [],
            "source": [
                "# Apply mappings\n",
                "post_survey_df['Participation'] = post_survey_df['Participation'].map(lambda x: participation_mapping.get(x, x))\n",
                "post_survey_df['Degree'] = post_survey_df['Degree'].map(lambda x: degree_mapping.get(x, x))\n",
                "post_survey_df['Subject'] = post_survey_df['Subject'].map(lambda x: subject_mapping.get(x, x))\n",
                "post_survey_df['Semester'] = post_survey_df['Semester'].map(lambda x: semester_mapping.get(x, x))\n",
                "post_survey_df['Age'] = post_survey_df['Age'].map(lambda x: age_mapping.get(x, x))\n",
                "post_survey_df['Gender'] = post_survey_df['Gender'].map(lambda x: gender_mapping.get(x, x))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6acf61a9a6be59d",
            "metadata": {},
            "source": [
                "## Filter the dataset to observations without missing values.\n",
                "To ensure the integrity of the analysis, we remove rows with missing values in critical columns. This step is essential to avoid skewed results due to incomplete data."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79ae477599bff284",
            "metadata": {},
            "source": [
                "### Remove Empty Rows\n",
                "Empty rows in the survey dataframes are surveys that were started but not submitted. We remove these rows to ensure that only completed surveys are analyzed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67e41d6e5c8c3dee",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.569324Z",
                    "start_time": "2025-09-27T16:58:56.565252Z"
                }
            },
            "outputs": [],
            "source": [
                "pre_survey_df = pre_survey_df.dropna(subset=[col for col in pre_survey_df.columns if col != \"user_id\"], how='all')\n",
                "post_survey_df = post_survey_df.dropna(subset=[col for col in post_survey_df.columns if col != \"user_id\"], how='all')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "706f593ab9d51fe",
            "metadata": {},
            "source": [
                "### Remove rows where _user_id_ is _NOT_FOUND_\n",
                "Rows where the user_id is NOT_FOUND cannot be mapped to a specific student. Therefore, these rows are removed from all datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e7bbcbe152ad2e57",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.688185Z",
                    "start_time": "2025-09-27T16:58:56.679268Z"
                }
            },
            "outputs": [],
            "source": [
                "competency_user_df = competency_user_df[competency_user_df[\"user_id\"] != \"NOT_FOUND\"].copy()\n",
                "participant_score_df = participant_score_df[participant_score_df[\"user_id\"] != \"NOT_FOUND\"].copy()\n",
                "participation_df = participation_df[participation_df[\"user_id\"] != \"NOT_FOUND\"].copy()\n",
                "pre_survey_df = pre_survey_df[pre_survey_df['user_id'] != 'NOT_FOUND'].copy()\n",
                "post_survey_df = post_survey_df[post_survey_df['user_id'] != 'NOT_FOUND'].copy()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f8b2207368f54f6",
            "metadata": {},
            "source": [
                "## Fill Missing Demographic Data in the Post-Survey from the Pre-Survey\n",
                "Students who have participated in the pre-survey were not asked the demographic questions again in the post-survey. To ensure that the demographic data is complete in the post-survey, we fill in any missing demographic information from the pre-survey based on the user_id."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9547f43bab9a6ae8",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.740515Z",
                    "start_time": "2025-09-27T16:58:56.734970Z"
                }
            },
            "outputs": [],
            "source": [
                "pre  = pre_survey_df.copy()\n",
                "post = post_survey_df.copy()\n",
                "\n",
                "# Demographic columns to transfer\n",
                "demo_cols = ['Degree', 'Subject', 'Semester', 'Age', 'Gender']\n",
                "\n",
                "# For each demographic column, build a lookup from pre and fill post\n",
                "# - drop_duplicates keeps the first occurrence per user_id\n",
                "for col in demo_cols:\n",
                "    lookup = pre.drop_duplicates('user_id').set_index('user_id')[col]\n",
                "    post[col] = post[col].fillna(post['user_id'].map(lookup))\n",
                "\n",
                "# Result back into the initial dataframe:\n",
                "post_survey_df = post"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7be55a4dde9ab878",
            "metadata": {},
            "source": [
                "## Function for Data Filtering\n",
                "For the following analysis, we often need to filter the science_event_df for specific event types. To streamline this process, we define a function that takes a list of event types and returns a filtered DataFrame containing only those events."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4173b4932ef8b021",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.791553Z",
                    "start_time": "2025-09-27T16:58:56.788635Z"
                }
            },
            "outputs": [],
            "source": [
                "## Filter table science_event for relevant data\n",
                "def filter_for_relevant_events(event_types):\n",
                "    \"\"\"\n",
                "    Filter the global `science_event_df` to rows matching the given event type(s).\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    event_types : sequence (list/tuple/array-like)\n",
                "        One or more event_type codes to keep (e.g., [5, 6, 7] or [5]).\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    pd.DataFrame\n",
                "        Subset of `science_event_df` containing only the requested event types.\n",
                "    \"\"\"\n",
                "    # Work on a copy to avoid modifying the original DataFrame\n",
                "    filtered = science_event_df.copy()\n",
                "\n",
                "    # For a single type, use equality; for multiple types, use .isin(...)\n",
                "    if len(event_types) == 1:\n",
                "        return filtered[filtered['event_type'] == event_types[0]]\n",
                "    else:\n",
                "        return filtered[filtered['event_type'].isin(event_types)]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b07a3d3cc502169",
            "metadata": {},
            "source": [
                "## Add Week Columns\n",
                "For temporal analyses, it is beneficial to have a 'week' column that indicates the week number of each event based on its timestamp. This function adds such a column to the DataFrame.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7694b7e6004f4f56",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:56.865724Z",
                    "start_time": "2025-09-27T16:58:56.863443Z"
                }
            },
            "outputs": [],
            "source": [
                "## Add week column\n",
                "def add_week(dataframe):\n",
                "    \"\"\"\n",
                "    Add an ISO week number column ('week') derived from a 'timestamp' column.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    dataframe : pd.DataFrame\n",
                "        Input table containing a 'timestamp' column (string, datetime, or pandas-compatible).\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    pd.DataFrame\n",
                "        The same DataFrame with an added 'week' column (ISO week: 1–53; dtype UInt32).\n",
                "    \"\"\"\n",
                "    # Ensure 'timestamp' is datetime\n",
                "    dataframe['timestamp'] = pd.to_datetime(dataframe['timestamp'])\n",
                "\n",
                "    # Extract ISO week number\n",
                "    dataframe['week'] = dataframe['timestamp'].dt.isocalendar().week\n",
                "\n",
                "    return dataframe\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6f18ddbe608a331f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:57.057186Z",
                    "start_time": "2025-09-27T16:58:56.914424Z"
                }
            },
            "outputs": [],
            "source": [
                "## Add week column where needed for later analysis\n",
                "science_event_df = add_week(science_event_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be03d74c496a8bec",
            "metadata": {},
            "source": [
                "## Map Exercises and Units to Competencies\n",
                "To facilitate the analyses that focus on competencies, we map exercises and lecture units to their corresponding competency IDs. This mapping allows us to analyze user interactions at the competency level."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a43fcf06f658ace",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:58:57.062945Z",
                    "start_time": "2025-09-27T16:58:57.060495Z"
                }
            },
            "outputs": [],
            "source": [
                "def resolve(row):\n",
                "    \"\"\"\n",
                "    Resolve a single event row to a competency_id based on its event_type.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    row : pd.Series\n",
                "        A row with at least 'event_type' and 'resource_id' fields.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    object or numpy.nan\n",
                "        The resolved competency_id if it can be determined; otherwise NaN.\n",
                "\n",
                "    Notes\n",
                "    -----\n",
                "    Uses global lookup maps:\n",
                "      - map_unit:      unit_id -> competency_id       (for event_type == 1)\n",
                "      - map_exercise:  exercise_id -> competency_id   (for event_type == 2)\n",
                "      - event_type in {3, 4}: resource_id is already a competency_id\n",
                "    \"\"\"\n",
                "    et, rid = row.event_type, row.resource_id\n",
                "\n",
                "    # Unit event -> lookup unit_id in map_unit\n",
                "    if et == 1:\n",
                "        return map_unit.get(rid, np.nan)\n",
                "\n",
                "    # Exercise event -> lookup exercise_id in map_exercise\n",
                "    elif et == 2:\n",
                "        return map_exercise.get(rid, np.nan)\n",
                "\n",
                "    # Already a competency event -> pass through the resource_id\n",
                "    elif et in (3, 4):\n",
                "        return rid\n",
                "\n",
                "    # Non-mappable event types -> no competency mapping, e.g. interactions with the learning path\n",
                "    else:\n",
                "        return np.nan\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b743bbb19abbacf",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.292017Z",
                    "start_time": "2025-09-27T16:58:57.106903Z"
                }
            },
            "outputs": [],
            "source": [
                "# Create mapping dictionaries\n",
                "map_unit     = competency_unit_mapping_df.set_index('lecture_unit_id')['competency_id'].to_dict()\n",
                "map_exercise = competency_exercise_mapping_df.set_index('exercise_id')['competency_id'].to_dict()\n",
                "\n",
                "science_event_df[\"competency_id\"] = science_event_df.apply(resolve, axis=1)\n",
                "science_event_df.to_csv(\"data/02_output/science_event_with_comp.csv\", index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d6c3aae17b437755",
            "metadata": {},
            "source": [
                "## General Data\n",
                "For the temporal analyses, we need to determine the start date of the course. This date serves as a reference point for calculating relative weeks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c3057bc99916f8b",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.391237Z",
                    "start_time": "2025-09-27T16:59:02.387872Z"
                }
            },
            "outputs": [],
            "source": [
                "start_date = pd.to_datetime(science_event_df['timestamp'].min())\n",
                "start_date"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a3ee2c9a94972669",
            "metadata": {},
            "source": [
                "## User Mapping\n",
                "To analyse the system and survey data based on user interactions, we create a comprehensive user mapping. This mapping includes all unique users from the relevant datasets and classifies them based on their interaction levels with the competencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f1ba9a8f42283a83",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.487296Z",
                    "start_time": "2025-09-27T16:59:02.440376Z"
                }
            },
            "outputs": [],
            "source": [
                "# Collect all unique user_ids from relevant datasets\n",
                "all_user_ids = pd.concat([\n",
                "    competency_user_df[['user_id']],\n",
                "    learning_path_df[['user_id']],\n",
                "    participant_score_df[['user_id']],\n",
                "    participation_df[['user_id']],\n",
                "    science_event_df[['user_id']]\n",
                "])\n",
                "\n",
                "# Remove the \"NOT_FOUND\" user_ids\n",
                "all_user_ids = all_user_ids.loc[all_user_ids['user_id'] != \"NOT_FOUND\"]\n",
                "\n",
                "# Remove duplicates to get unique user_ids\n",
                "unique_users_df = all_user_ids.drop_duplicates().reset_index(drop=True)\n",
                "\n",
                "# Show the number of unique users\n",
                "print(f\"Number of unique users: {len(unique_users_df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c17f507f83e298f7",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.529639Z",
                    "start_time": "2025-09-27T16:59:02.527601Z"
                }
            },
            "outputs": [],
            "source": [
                "def classify_interaction_group(count):\n",
                "    \"\"\"\n",
                "    Classify a user into an interaction group based on their interaction count.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    count : int or float\n",
                "        Number of interactions observed for the user.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    str\n",
                "        One of {'No', 'Low', 'High'}:\n",
                "          - 'No'   if count == 0\n",
                "          - 'Low'  if 1 <= count <= median_interaction_count\n",
                "          - 'High' if count  >  median_interaction_count\n",
                "    \"\"\"\n",
                "    if count == 0:\n",
                "        return 'No'\n",
                "    elif count <= median_interaction_count:\n",
                "        return 'Low'\n",
                "    else:\n",
                "        return 'High'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "275a04c9dfe9fb80",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.599187Z",
                    "start_time": "2025-09-27T16:59:02.574658Z"
                }
            },
            "outputs": [],
            "source": [
                "# Collect interactions with the competencies per user to classify users into groups\n",
                "# We focus on event_type 3 (Competency open)\n",
                "\n",
                "# Filter for relevant events\n",
                "relevant_event_types = [3]  # Event type 3 = Competency open\n",
                "interaction_competencies_df = filter_for_relevant_events(relevant_event_types)\n",
                "\n",
                "# Count interactions per user\n",
                "interaction_counts = interaction_competencies_df['user_id'].value_counts().to_dict()\n",
                "\n",
                "# Add interaction counts to unique_users_df\n",
                "unique_users_df['interaction_count'] = unique_users_df['user_id'].apply(lambda uid: interaction_counts.get(uid, 0))\n",
                "\n",
                "# Filter out users with zero interactions for median calculation\n",
                "active_users_df = unique_users_df[unique_users_df['interaction_count'] > 0]\n",
                "\n",
                "# Calculate general statistics about interaction count among active users\n",
                "min_interaction_count = active_users_df['interaction_count'].min()\n",
                "print(f\"Minimum interaction of users with the competencies: {min_interaction_count}\")\n",
                "max_interaction_count = active_users_df['interaction_count'].max()\n",
                "print(f\"Maximum interaction of users with the competencies: {max_interaction_count}\")\n",
                "mean_interaction_count = active_users_df['interaction_count'].mean()\n",
                "print(f\"Mean interaction of users with the competencies: {mean_interaction_count}\")\n",
                "sd_interaction_count = active_users_df['interaction_count'].std()\n",
                "print(f\"Standard deviation interaction of users with the competencies: {sd_interaction_count}\")\n",
                "\n",
                "\n",
                "# Calculate the median interaction count among active users\n",
                "median_interaction_count = active_users_df['interaction_count'].median()\n",
                "print(f\"Median interaction of users with the competencies: {median_interaction_count}\")\n",
                "\n",
                "unique_users_df['interaction_group'] = unique_users_df['interaction_count'].apply(classify_interaction_group)\n",
                "unique_users_df.to_csv('data/01_anonymized/user_mapping.csv', index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3731c395f0d91792",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.603694Z",
                    "start_time": "2025-09-27T16:59:02.601333Z"
                }
            },
            "outputs": [],
            "source": [
                "# Count the number of users per interaction group\n",
                "number_of_users_group_no = (unique_users_df['interaction_group'] == 'No').sum()\n",
                "number_of_users_group_low = (unique_users_df['interaction_group'] == 'Low').sum()\n",
                "number_of_users_group_high = (unique_users_df['interaction_group'] == 'High').sum()\n",
                "\n",
                "print(\"Number of users in the 'No' group:\", number_of_users_group_no)\n",
                "print(\"Number of users in the 'Low' group:\", number_of_users_group_low)\n",
                "print(\"Number of users in the 'High'group:\", number_of_users_group_high)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "411ad8c9e957f4c0",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.645648Z",
                    "start_time": "2025-09-27T16:59:02.643292Z"
                }
            },
            "outputs": [],
            "source": [
                "def add_interaction_group(df):\n",
                "    \"\"\"\n",
                "    Add each user's interaction group and a binary 'interaction' flag to the input DataFrame.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    df : pd.DataFrame\n",
                "        Table that contains a 'user_id' column to join on.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    pd.DataFrame\n",
                "        A copy of `df` with:\n",
                "          - 'interaction_group' joined from `unique_users_df`\n",
                "          - 'interaction' flag: 'yes' for High/Low, 'no' otherwise (incl. NaN)\n",
                "    \"\"\"\n",
                "    # Two-column mapping from the user registry\n",
                "    group_mapping = unique_users_df[[\"user_id\", \"interaction_group\"]]\n",
                "\n",
                "    # Left join: keep all rows from df; attach group where available\n",
                "    df = df.merge(group_mapping, on=\"user_id\", how=\"left\")\n",
                "\n",
                "    # Derive a simple interaction flag from the group label\n",
                "    df[\"interaction\"] = np.where(df[\"interaction_group\"].isin([\"High\", \"Low\"]), \"yes\", \"no\")\n",
                "\n",
                "    return df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4971461931102a41",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.876208Z",
                    "start_time": "2025-09-27T16:59:02.723525Z"
                }
            },
            "outputs": [],
            "source": [
                "# Add the interaction group to all relevant datasets\n",
                "exam_score_df = add_interaction_group(exam_score_df)\n",
                "science_event_df = add_interaction_group(science_event_df)\n",
                "participant_score_df = add_interaction_group(participant_score_df)\n",
                "post_survey_df = add_interaction_group(post_survey_df)\n",
                "competency_user_df = add_interaction_group(competency_user_df)\n",
                "participation_df = add_interaction_group(participation_df)\n",
                "learning_path_df = add_interaction_group(learning_path_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6e4085258dab58ac",
            "metadata": {},
            "source": [
                "# Data Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6279944d9209fc1",
            "metadata": {},
            "source": [
                "## Demographics Analysis\n",
                "We start the analysis by examining the demographic characteristics of the survey participants. This includes summarizing key variables such as degree program, subject area, semester, age"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ccb19a55a782172",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.922904Z",
                    "start_time": "2025-09-27T16:59:02.919692Z"
                }
            },
            "outputs": [],
            "source": [
                "def make_table(pre: pd.DataFrame, post: pd.DataFrame, variables):\n",
                "    \"\"\"\n",
                "    Build a simple summary table for selected variables with counts and percentages\n",
                "    for pre- and post-survey data.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    pre, post : pd.DataFrame\n",
                "        DataFrames containing the variables to summarize.\n",
                "    variables : list-like\n",
                "        Column names to include (e.g., [\"Degree\", \"Subject\", \"Semester\"]).\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    pd.DataFrame\n",
                "        A two-column summary per variable showing \"count (percent)\" for pre/post.\n",
                "    \"\"\"\n",
                "    rows = []\n",
                "    for var in variables:\n",
                "        # Take non-missing values for the current variable\n",
                "        pre_s, post_s = pre[var].dropna(), post[var].dropna()\n",
                "        n_pre, n_post = len(pre_s), len(post_s)\n",
                "\n",
                "        # Frequency counts per response level\n",
                "        c_pre, c_post = pre_s.value_counts(), post_s.value_counts()\n",
                "\n",
                "        # All levels seen in either pre or post; keep the order as they first appear\n",
                "        levels = list(dict.fromkeys(list(c_pre.index) + list(c_post.index)))\n",
                "\n",
                "        # Add a header row for this variable (blank cells for readability)\n",
                "        rows.append({\"Variable\": var, \"Pre-Survey\": \"\", \"Post-Survey\": \"\"})\n",
                "\n",
                "        # Add one row per level with \"count (percent)\" for pre and post\n",
                "        for lvl in levels:\n",
                "            p = int(c_pre.get(lvl, 0))   # pre count for this level (0 if absent)\n",
                "            q = int(c_post.get(lvl, 0))  # post count for this level (0 if absent)\n",
                "\n",
                "            # Percentages are relative to non-missing counts; guard against n=0\n",
                "            pre_cell  = f\"{p} ({(p/n_pre*100 if n_pre else 0):.2f}%)\"\n",
                "            post_cell = f\"{q} ({(q/n_post*100 if n_post else 0):.2f}%)\"\n",
                "\n",
                "            rows.append({\n",
                "                \"Variable\": f\"  {lvl}\",  # indent level label under the variable\n",
                "                \"Pre-Survey\": pre_cell,\n",
                "                \"Post-Survey\": post_cell\n",
                "            })\n",
                "\n",
                "    return pd.DataFrame(rows)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a99d732a3bdd63cf",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:02.986307Z",
                    "start_time": "2025-09-27T16:59:02.979722Z"
                }
            },
            "outputs": [],
            "source": [
                "# Create Demographics Table\n",
                "vars_ = [\"Degree\", \"Subject\", \"Semester\", \"Age\", \"Gender\"]\n",
                "table = make_table(pre_survey_df, post_survey_df, vars_)\n",
                "table"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c06811f2d2443c83",
            "metadata": {},
            "source": [
                "## Descriptive Analysis\n",
                "The evaluation section addresses the core research questions of the thesis. The evaluation is structured into the three main areas of interest, each corresponding to a specific research question: Perception, Interaction, and effect on exercise engagement and completion."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "125aefdad7265887",
            "metadata": {},
            "source": [
                "### Performance\n",
                "*Research Question 1:* How does the level of interaction with CBETool relate to student performance on assessments?"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9bf41324a437dfa",
            "metadata": {},
            "source": [
                "#### Mastery Score\n",
                "We analyse the mastery score as a measure of performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "66f15ed4a6bba417",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.064659Z",
                    "start_time": "2025-09-27T16:59:03.037968Z"
                }
            },
            "outputs": [],
            "source": [
                "# Build the Cartesian product of all users × all competencies\n",
                "all_combinations = unique_users_df.merge(\n",
                "    competency_df,\n",
                "    how=\"cross\"\n",
                ")\n",
                "# Rename the generic competency 'id' to 'competency_id' for clarity and joins\n",
                "all_combinations = all_combinations.rename(columns={'id': 'competency_id'})\n",
                "\n",
                "# Merge with existing user–competency records to bring in progress/confidence where available\n",
                "full_df = all_combinations.merge(\n",
                "    competency_user_df,\n",
                "    on=[\"user_id\", \"competency_id\"],\n",
                "    how=\"left\"  # keep every user–competency pair even if no record exists yet\n",
                ")\n",
                "\n",
                "# Fill missing progress/confidence with 0 (interpreted as not started / no confidence reported)\n",
                "full_df[\"progress\"] = full_df[\"progress\"].fillna(0)\n",
                "full_df[\"confidence\"] = full_df[\"confidence\"].fillna(0)\n",
                "\n",
                "# Remove duplicate interaction_group columns produced by merges\n",
                "full_df = full_df.drop(columns=[\"interaction_group_x\", \"interaction_group_y\"])\n",
                "\n",
                "# Compute a single interaction_group column based on the current data\n",
                "full_df = add_interaction_group(full_df)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dfb00d9bef0f6359",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.239913Z",
                    "start_time": "2025-09-27T16:59:03.111732Z"
                }
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Set global font sizes\n",
                "plt.rcParams.update({\n",
                "    \"font.size\": 12,\n",
                "    \"axes.titlesize\": 18,\n",
                "    \"axes.labelsize\": 14,\n",
                "    \"xtick.labelsize\": 12,\n",
                "    \"ytick.labelsize\": 12\n",
                "})\n",
                "\n",
                "# Column names and styling\n",
                "USER_COL = 'user_id'\n",
                "GROUP_COL = 'interaction_group'\n",
                "COMP_COL  = 'competency_id'\n",
                "GROUP_ORDER  = ['No', 'Low', 'High']\n",
                "GROUP_COLORS = {'No': '#E0703C', 'Low': '#4C78A8', 'High': '#7A5195'}\n",
                "LABEL_MAP = {'No': 'N-Group', 'Low': 'L-Group', 'High': 'H-Group'}\n",
                "\n",
                "# Map each user to a single interaction group\n",
                "group_map = (full_df[[USER_COL, GROUP_COL]]\n",
                "             .drop_duplicates(subset=[USER_COL])\n",
                "             .set_index(USER_COL)[GROUP_COL])\n",
                "\n",
                "# Compute mastery and normalized progress (0–1)\n",
                "full_df['mastery'] = (full_df['progress'] * full_df['confidence']).clip(0, 100)\n",
                "full_df['mastery_progress'] = (full_df['mastery'] / full_df['mastery_threshold']).clip(0, 1)\n",
                "\n",
                "# For each (user, competency) take max; then average per user\n",
                "max_mp_per_comp = full_df.groupby([USER_COL, COMP_COL], as_index=False)['mastery_progress'].max()\n",
                "avg_mp_per_user = max_mp_per_comp.groupby(USER_COL)['mastery_progress'].mean().rename('avg_mastery_progress')\n",
                "\n",
                "# Join averages with group labels\n",
                "plot_df = avg_mp_per_user.to_frame().join(group_map, how='inner')\n",
                "\n",
                "# Prepare per-group arrays in percent\n",
                "vals_by_group = {\n",
                "    g: (plot_df.loc[plot_df[GROUP_COL] == g, 'avg_mastery_progress'].values * 100.0)\n",
                "    for g in GROUP_ORDER\n",
                "}\n",
                "vals_by_group = {g: v for g, v in vals_by_group.items() if v.size > 0}\n",
                "\n",
                "# Half-violin plot\n",
                "fig, ax = plt.subplots(figsize=(7.5, 5.2))\n",
                "ypos = {g: i for i, g in enumerate(vals_by_group.keys())}\n",
                "data = [vals_by_group[g] for g in vals_by_group.keys()]\n",
                "pos  = [ypos[g] for g in vals_by_group.keys()]\n",
                "\n",
                "vp = ax.violinplot(dataset=data, positions=pos, vert=False,\n",
                "                   showmeans=False, showmedians=False, showextrema=False, widths=0.9)\n",
                "\n",
                "# Keep upper half and color by group\n",
                "for body, g in zip(vp['bodies'], vals_by_group.keys()):\n",
                "    verts = body.get_paths()[0].vertices\n",
                "    y0 = ypos[g]\n",
                "    verts[:, 1] = np.maximum(verts[:, 1], y0)\n",
                "    body.set_facecolor(GROUP_COLORS.get(g, '0.5'))\n",
                "    body.set_edgecolor('none')\n",
                "    body.set_alpha(0.35)\n",
                "\n",
                "# Add mean tick and label per group\n",
                "for g, v in vals_by_group.items():\n",
                "    y = ypos[g]\n",
                "    mean = float(np.mean(v))\n",
                "    ax.plot([mean, mean], [y, y + 0.36],\n",
                "            linewidth=2.4, color=GROUP_COLORS.get(g, '0.5'), zorder=5)\n",
                "    ax.text(mean, y + 0.40, f\"Mean = {mean:.1f}%\",\n",
                "            ha='center', va='bottom', fontsize=12,\n",
                "            color=GROUP_COLORS.get(g, '0.5'),\n",
                "            bbox=dict(facecolor='white', alpha=0.75, edgecolor='none', pad=1.5))\n",
                "\n",
                "# Axis labels and grid\n",
                "ax.set_yticks(list(ypos.values()))\n",
                "ax.set_yticklabels([LABEL_MAP[g] for g in vals_by_group.keys()])\n",
                "ax.set_xlim(0, 100)\n",
                "ax.set_xlabel('Average Mastery Progress per User (%)')\n",
                "ax.grid(True, axis='x', alpha=0.3)\n",
                "\n",
                "# Annotate N per group\n",
                "xmin = ax.get_xlim()[0]; pad = 2\n",
                "for g, v in vals_by_group.items():\n",
                "    ax.text(xmin + pad, ypos[g] + 0.42, f\"N={len(v)}\",\n",
                "            color=GROUP_COLORS.get(g, '0.5'),\n",
                "            va='bottom', ha='left', fontsize=11)\n",
                "\n",
                "fig.tight_layout()\n",
                "\n",
                "# Save and show\n",
                "out_path = \"data/02_output/mastery.pdf\"\n",
                "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
                "fig.savefig(out_path, format=\"pdf\", bbox_inches=\"tight\")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a282218382452c12",
            "metadata": {},
            "source": [
                "#### Exam Score\n",
                "We analyse the exam score as a measure of performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e93db37c8b20f4b",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.391817Z",
                    "start_time": "2025-09-27T16:59:03.281969Z"
                }
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Set global font sizes for consistent styling\n",
                "plt.rcParams.update({\n",
                "    \"font.size\": 12,\n",
                "    \"axes.titlesize\": 18,\n",
                "    \"axes.labelsize\": 14,\n",
                "    \"xtick.labelsize\": 12,\n",
                "    \"ytick.labelsize\": 12\n",
                "})\n",
                "\n",
                "# Column names and plotting style\n",
                "USER_COL   = 'user_id'\n",
                "GROUP_COL  = 'interaction_group'\n",
                "POINTS_COL = 'last_rated_points'\n",
                "GROUP_ORDER  = ['No', 'Low', 'High']\n",
                "GROUP_COLORS = {'No': '#E0703C', 'Low': '#4C78A8', 'High': '#7A5195'}\n",
                "LABEL_MAP = {'No': 'N-Group', 'Low': 'L-Group', 'High': 'H-Group'}\n",
                "\n",
                "# Exam scoring constants and pass threshold (in %)\n",
                "TOTAL_POINTS = 120\n",
                "PASS_POINTS  = 45\n",
                "PASS_PCT     = 100 * PASS_POINTS / TOTAL_POINTS  # 37.5\n",
                "\n",
                "# Total exam score per user; keep group label\n",
                "df = exam_score_df[[USER_COL, GROUP_COL, POINTS_COL]].copy()\n",
                "df[POINTS_COL] = df[POINTS_COL].fillna(0)\n",
                "\n",
                "# Sum points across items per user, then convert to percentage\n",
                "total_exam = df.groupby(USER_COL, as_index=True)[POINTS_COL].sum().rename('total_exam_score')\n",
                "total_exam_pct = (total_exam / TOTAL_POINTS * 100).rename('total_exam_pct')\n",
                "\n",
                "# Attach each user's group to the percentage scores\n",
                "groups  = df.drop_duplicates(subset=[USER_COL]).set_index(USER_COL)[GROUP_COL]\n",
                "plot_df = total_exam_pct.to_frame().join(groups, how='inner')\n",
                "\n",
                "# Build arrays of percentages by group\n",
                "vals_by_group = {\n",
                "    g: plot_df.loc[plot_df[GROUP_COL] == g, 'total_exam_pct'].values\n",
                "    for g in GROUP_ORDER\n",
                "}\n",
                "vals_by_group = {g: v for g, v in vals_by_group.items() if v.size > 0}\n",
                "\n",
                "# Half-violin plot (horizontal)\n",
                "fig, ax = plt.subplots(figsize=(7.5, 5.2))\n",
                "ypos = {g: i for i, g in enumerate(vals_by_group.keys())}\n",
                "data = [vals_by_group[g] for g in vals_by_group.keys()]\n",
                "pos  = [ypos[g] for g in vals_by_group.keys()]\n",
                "\n",
                "vp = ax.violinplot(dataset=data, positions=pos, vert=False,\n",
                "                   showmeans=False, showmedians=False, showextrema=False, widths=0.9)\n",
                "\n",
                "# Keep upper half of each violin and color by group\n",
                "for body, g in zip(vp['bodies'], vals_by_group.keys()):\n",
                "    verts = body.get_paths()[0].vertices\n",
                "    y0 = ypos[g]\n",
                "    verts[:, 1] = np.maximum(verts[:, 1], y0)\n",
                "    body.set_facecolor(GROUP_COLORS[g])\n",
                "    body.set_edgecolor('none')\n",
                "    body.set_alpha(0.35)\n",
                "\n",
                "# Axes, labels, and grid (0–100%)\n",
                "ax.set_yticks(list(ypos.values()))\n",
                "ax.set_yticklabels([LABEL_MAP[g] for g in vals_by_group.keys()])\n",
                "ax.set_xlim(0, 100)\n",
                "ax.set_xlabel('Total Exam Score per User (%)')\n",
                "ax.grid(True, axis='x', alpha=0.3)\n",
                "\n",
                "# Passing threshold line and label\n",
                "line = ax.axvline(PASS_PCT, linestyle='--', linewidth=1.8)\n",
                "ymin, ymax = ax.get_ylim()\n",
                "label_y = ymin + 0.02 * (ymax - ymin)\n",
                "ax.text(\n",
                "    PASS_PCT, label_y, f'Passing threshold ({PASS_PCT:.1f}%)',\n",
                "    ha='center', va='bottom', rotation=0, color=line.get_color(),\n",
                "    fontsize=11,\n",
                "    bbox=dict(facecolor='white', alpha=0.75, edgecolor='none', pad=1.5)\n",
                ")\n",
                "\n",
                "# Mean tick per group with label placed near the tick\n",
                "mean_tick_height = 0.60\n",
                "offset = 2.0\n",
                "xmin, xmax = ax.get_xlim()\n",
                "\n",
                "for g, vals in vals_by_group.items():\n",
                "    y = ypos[g]\n",
                "    mean = float(np.mean(vals))\n",
                "    ax.plot([mean, mean], [y, y + mean_tick_height],\n",
                "            linewidth=2.6, color=GROUP_COLORS[g], zorder=6)\n",
                "\n",
                "    # Place label to the right or left of the tick\n",
                "    if g in ('High', 'Low'):\n",
                "        x_label = min(mean + offset, xmax - 1)\n",
                "        ha = 'left'\n",
                "    else:\n",
                "        x_label = max(mean - offset, xmin + 1)\n",
                "        ha = 'right'\n",
                "\n",
                "    ax.text(x_label, y + mean_tick_height + 0.035,\n",
                "            f\"Mean = {mean:.1f}%\",\n",
                "            ha=ha, va='bottom', fontsize=12,\n",
                "            color=GROUP_COLORS[g],\n",
                "            bbox=dict(facecolor='white', alpha=0.80, edgecolor='none', pad=1.5))\n",
                "\n",
                "# Annotate sample size per group\n",
                "pad = 2\n",
                "for g, v in vals_by_group.items():\n",
                "    ax.text(xmin + pad, ypos[g] + 0.42, f\"N={len(v)}\",\n",
                "            color=GROUP_COLORS[g], va='bottom', ha='left', fontsize=11)\n",
                "\n",
                "fig.tight_layout()\n",
                "\n",
                "# Save to PDF and show\n",
                "out_path = \"data/02_output/exam.pdf\"\n",
                "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
                "fig.savefig(out_path, format=\"pdf\", bbox_inches=\"tight\")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "508fadac2b01d9ff",
            "metadata": {},
            "source": [
                "#### Passing Rate in the Exam\n",
                "We analyse the passing rate as a measure of performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "211d8b4b5e0e8949",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.447288Z",
                    "start_time": "2025-09-27T16:59:03.437166Z"
                }
            },
            "outputs": [],
            "source": [
                "# Set relevant variables\n",
                "USER_COL   = 'user_id'\n",
                "GROUP_COL  = 'interaction_group'\n",
                "POINTS_COL = 'last_rated_points'\n",
                "GROUP_ORDER = ['High', 'Low', 'No']\n",
                "PASSING_THRESHOLD = 45\n",
                "\n",
                "# Total exam score per user\n",
                "df = exam_score_df[[USER_COL, GROUP_COL, POINTS_COL]].copy()\n",
                "df[POINTS_COL] = df[POINTS_COL].fillna(0)\n",
                "\n",
                "total_exam = df.groupby(USER_COL, as_index=True)[POINTS_COL].sum().rename('total_exam_score')\n",
                "groups     = df.drop_duplicates(subset=[USER_COL]).set_index(USER_COL)[GROUP_COL]\n",
                "\n",
                "users = total_exam.to_frame().join(groups, how='inner').reset_index()\n",
                "\n",
                "# Evaluate whether the user passed\n",
                "users['passed'] = users['total_exam_score'] >= PASSING_THRESHOLD\n",
                "\n",
                "# Percent passed per group\n",
                "summary = (users.groupby(GROUP_COL)\n",
                "           .agg(n_users=(USER_COL, 'size'),\n",
                "                passed_n=('passed', 'sum'))\n",
                "           .reindex(GROUP_ORDER))\n",
                "summary['passed_pct'] = (summary['passed_n'] / summary['n_users'] * 100).round(1)\n",
                "\n",
                "print(summary)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b640d149db284d94",
            "metadata": {},
            "source": [
                "### Engagement and Motivation\n",
                "*Research Question 2:* How does the level of interaction with CBETool relate to student engagement and motivation?"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26d3b0acc3df6586",
            "metadata": {},
            "source": [
                "#### Practice Mode\n",
                "We analyse the usage of the practice mode as a measure of engagement and motivation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9e3e7c8990d755d",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.546466Z",
                    "start_time": "2025-09-27T16:59:03.525234Z"
                }
            },
            "outputs": [],
            "source": [
                "df = participation_df.copy()\n",
                "\n",
                "# Total number of test runs per interaction group (booleans sum with True=1)\n",
                "runs_per_group = df.groupby(\"interaction_group\")[\"test_run\"].sum()\n",
                "\n",
                "# External group sizes (number of users in each group)\n",
                "group_sizes = pd.Series({\n",
                "    \"No\": number_of_users_group_no,\n",
                "    \"Low\": number_of_users_group_low,\n",
                "    \"High\": number_of_users_group_high,\n",
                "})\n",
                "\n",
                "# Average test runs per user = total runs / users (by group)\n",
                "avg_per_group_all_users = (\n",
                "    runs_per_group.reindex([\"No\", \"Low\", \"High\"])                  # consistent order\n",
                "                 .div(group_sizes.reindex([\"No\", \"Low\", \"High\"]))  # divide by group user counts\n",
                "                 .replace([np.inf, -np.inf], np.nan)               # guard against division by zero\n",
                "                 .fillna(0)\n",
                "                 .round(2)\n",
                ")\n",
                "\n",
                "print(\"Average Number of Test Runs Handed in by a User per Interaction Group:\")\n",
                "print(avg_per_group_all_users)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7b217836aaab0b22",
            "metadata": {},
            "source": [
                "#### Learning Path\n",
                "We analyse the interaction with the learning path as a measure of engagement and motivation. We use two metrics: (1) the percentage of users who started the learning path and (2) the average number of interactions with the learning path per user."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f1345a6afac0af06",
            "metadata": {},
            "source": [
                "##### Users Who Started the Learning Path\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a26f69130a5332cf",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.713424Z",
                    "start_time": "2025-09-27T16:59:03.679165Z"
                }
            },
            "outputs": [],
            "source": [
                "# Count users who started the learning path per interaction group\n",
                "started_counts = learning_path_df[learning_path_df['started_by_student'] == 1].groupby('interaction_group')['user_id'].nunique()\n",
                "\n",
                "# Count total users per interaction group\n",
                "lp_total_counts = learning_path_df.groupby('interaction_group')['user_id'].nunique()\n",
                "\n",
                "# Calculate percentage\n",
                "percentage_started = (started_counts / lp_total_counts * 100).round(2)\n",
                "\n",
                "# Combine into a DataFrame for readability\n",
                "lp_started_summary = pd.DataFrame({\n",
                "    'users_started': started_counts,\n",
                "    'total_users': lp_total_counts,\n",
                "    'percent_started': percentage_started\n",
                "})\n",
                "\n",
                "print(lp_started_summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "649e62259ee74042",
            "metadata": {},
            "source": [
                "##### Interactions with the Learning Path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0cc3d2ffddb3fde",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.921403Z",
                    "start_time": "2025-09-27T16:59:03.799323Z"
                }
            },
            "outputs": [],
            "source": [
                "lp_interactions = science_event_df.copy()\n",
                "\n",
                "# Convert event_type to numeric safely and keep only relevant types\n",
                "lp_interactions[\"event_type\"] = pd.to_numeric(lp_interactions[\"event_type\"], errors=\"coerce\")\n",
                "lp_interactions = lp_interactions[lp_interactions[\"event_type\"].isin([5, 6, 7, 8])]\n",
                "\n",
                "# Count interactions per (group, user)\n",
                "lp_interactions_per_user = (\n",
                "    lp_interactions.groupby([\"interaction_group\", \"user_id\"], as_index=False)\n",
                "      .size()\n",
                "      .rename(columns={\"size\": \"interactions\"})\n",
                ")\n",
                "\n",
                "# Average interactions per group (among users who interacted)\n",
                "avg_per_group = (\n",
                "    lp_interactions_per_user.groupby(\"interaction_group\")[\"interactions\"]\n",
                "        .mean()\n",
                "        .round(2)\n",
                "        .reindex([\"High\", \"Low\", \"No\"])\n",
                ")\n",
                "\n",
                "print(\"Average interactions per interacting user per group: \")\n",
                "print(avg_per_group)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6a898a28920032b9",
            "metadata": {},
            "source": [
                "### Perception of the System\n",
                "*Research Question 3:* How do students perceive CBETool in terms of usability and learning support?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fbdb7ae2c1b713e1",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:03.938964Z",
                    "start_time": "2025-09-27T16:59:03.931746Z"
                }
            },
            "outputs": [],
            "source": [
                "# Map numeric Likert codes to textual labels\n",
                "plot_likert_mapping = {\n",
                "    5: \"Strongly agree\",\n",
                "    4: \"Agree\",\n",
                "    3: \"Neither agree nor disagree\",\n",
                "    2: \"Disagree\",\n",
                "    1: \"Strongly disagree\",\n",
                "}\n",
                "\n",
                "# Custom bar colors (left → right). The first color is transparent.\n",
                "bar_colors = [\"#ffffff00\", \"#f5a7a7\", \"#FFA07A\", \"#D3D3D3\", \"#9cd7fb\", \"#55baf8\"]\n",
                "\n",
                "def plot_likert(data, figsize=(10, 3), title=''):\n",
                "    \"\"\"\n",
                "    Plot a Likert-style stacked bar chart (in percentages) from 1–5 responses.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    data : pd.DataFrame or pd.Series\n",
                "        Table/series of numeric Likert responses (1–5). Each column is an item.\n",
                "    figsize : tuple, default (10, 3)\n",
                "        Figure size passed to the plotting backend.\n",
                "    title : str, default ''\n",
                "        Title placed above the chart.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    matplotlib.axes.Axes\n",
                "        The Axes object containing the Likert plot.\n",
                "    \"\"\"\n",
                "    # Map numeric responses (1–5) to textual Likert labels\n",
                "    data_for_plot = data.map(plot_likert_mapping.get)\n",
                "\n",
                "    # Draw stacked bars as percentages on the canonical 5-point 'agree' scale\n",
                "    axes = pl.plot_likert(\n",
                "        df=data_for_plot,\n",
                "        plot_scale=pl.scales.agree,\n",
                "        plot_percentage=True,\n",
                "        colors=bar_colors,\n",
                "        figsize=figsize,\n",
                "        xtick_interval=10\n",
                "    )\n",
                "\n",
                "    # Basic cosmetics: title, compact legend under the plot, no background grid\n",
                "    axes.set_title(title)\n",
                "    axes.legend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.5))\n",
                "    axes.grid(False)\n",
                "\n",
                "    # Add centered percentage labels for each non-zero segment\n",
                "    for bars in axes.containers[1:]:\n",
                "        axes.bar_label(\n",
                "            bars,\n",
                "            label_type='center',\n",
                "            fmt=lambda x: f'{x:.1f}%' if x != 0 else '',\n",
                "            fontsize=11\n",
                "        )\n",
                "\n",
                "    # Save a publication-ready PDF\n",
                "    out_path = \"data/02_output/perception.pdf\"\n",
                "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
                "    fig = axes.get_figure()\n",
                "    fig.savefig(out_path, format=\"pdf\", bbox_inches=\"tight\")\n",
                "\n",
                "    return axes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "799fe54f97be6262",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:04.453871Z",
                    "start_time": "2025-09-27T16:59:04.091766Z"
                }
            },
            "outputs": [],
            "source": [
                "# Map categorical codes (AO01–AO05) to numeric Likert scores 1–5\n",
                "mapping = {'AO01': 1.0, 'AO02': 2.0, 'AO03': 3.0, 'AO04': 4.0, 'AO05': 5.0}\n",
                "\n",
                "# Recode each survey scale to numeric and ensure float dtype\n",
                "post_survey_df['Usability']     = post_survey_df['Usability'].replace(mapping).astype(float)\n",
                "post_survey_df['Transparency']  = post_survey_df['Transparency'].replace(mapping).astype(float)\n",
                "post_survey_df['Confidence']    = post_survey_df['Confidence'].replace(mapping).astype(float)\n",
                "post_survey_df['Frustration']   = post_survey_df['Frustration'].replace(mapping).astype(float)\n",
                "\n",
                "# Keep only respondents with interaction == \"yes\" (case-insensitive) and select target columns -> only students who interacted with a system can tell us about their perception of it\n",
                "data = post_survey_df.loc[\n",
                "    post_survey_df[\"interaction\"].astype(str).str.lower().eq(\"yes\"),\n",
                "    [\"Usability\", \"Transparency\", \"Confidence\", \"Frustration\"]\n",
                "].copy()\n",
                "\n",
                "label_map = {\n",
                "    \"Usability\": \"Easy to use\",\n",
                "    \"Transparency\": \"Provides transparency\",\n",
                "    \"Confidence\": \"Confident about passing\",\n",
                "    \"Frustration\": \"Frustrated by the course\"\n",
                "}\n",
                "\n",
                "# Apply renaming to the dataframe\n",
                "data = data.rename(columns=label_map)\n",
                "\n",
                "# Draw the Likert chart using the helper defined earlier\n",
                "plot_likert(data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "osyuv6gsi3",
            "metadata": {},
            "source": [
                "#### Frustration and Confidence Relationship\n",
                "\n",
                "To better understand the intriguing tension between frustration and confidence reported in the survey, we examine the relationship between these two constructs. This analysis helps clarify whether frustration stems from the tool itself or from the inherent difficulty of the course content, and how the tool may support self-efficacy despite challenging course demands."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "91f30r04y3e",
            "metadata": {},
            "source": [
                "##### Correlation Analysis\n",
                "\n",
                "First, we examine the correlation between Confidence and Frustration scores to understand whether these constructs are inversely related, independent, or show a different pattern."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "yg5652trd59",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for correlation analysis\n",
                "# Use the same filtered data as the Likert plot (interaction == \"yes\")\n",
                "correlation_data = post_survey_df.loc[\n",
                "    post_survey_df[\"interaction\"].astype(str).str.lower().eq(\"yes\"),\n",
                "    [\"Confidence\", \"Frustration\", \"Usability\", \"Transparency\"]\n",
                "].copy()\n",
                "\n",
                "# Remove any rows with missing values in these columns\n",
                "correlation_data = correlation_data.dropna()\n",
                "\n",
                "# Calculate Spearman correlation (appropriate for Likert scale data)\n",
                "correlation_result = pg.corr(\n",
                "    correlation_data['Confidence'], \n",
                "    correlation_data['Frustration'],\n",
                "    method='spearman'\n",
                ")\n",
                "\n",
                "print(\"Spearman Correlation between Confidence and Frustration:\")\n",
                "print(correlation_result)\n",
                "print(f\"\\nInterpretation:\")\n",
                "print(f\"  - Spearman's ρ = {correlation_result['r'].values[0]:.3f}\")\n",
                "print(f\"  - p-value = {correlation_result['p-val'].values[0]:.4f}\")\n",
                "print(f\"  - n = {correlation_result['n'].values[0]}\")\n",
                "\n",
                "if correlation_result['p-val'].values[0] < 0.05:\n",
                "    if abs(correlation_result['r'].values[0]) < 0.3:\n",
                "        strength = \"weak\"\n",
                "    elif abs(correlation_result['r'].values[0]) < 0.5:\n",
                "        strength = \"moderate\"\n",
                "    else:\n",
                "        strength = \"strong\"\n",
                "    \n",
                "    direction = \"negative\" if correlation_result['r'].values[0] < 0 else \"positive\"\n",
                "    print(f\"\\nThere is a statistically significant {strength} {direction} correlation between Confidence and Frustration.\")\n",
                "else:\n",
                "    print(f\"\\nNo statistically significant correlation between Confidence and Frustration was found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "wd7ht51e2me",
            "metadata": {},
            "source": [
                "##### Cross-Tabulation of High Confidence and High Frustration\n",
                "\n",
                "Next, we examine how many students simultaneously report both high confidence (agree or strongly agree) and high frustration (agree or strongly agree), which represents the core of the \"intriguing tension\" noted in the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "nxu4z3jk1za",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create binary variables for high confidence and high frustration\n",
                "# High = Agree (4) or Strongly Agree (5)\n",
                "correlation_data['high_confidence'] = correlation_data['Confidence'] >= 4\n",
                "correlation_data['high_frustration'] = correlation_data['Frustration'] >= 4\n",
                "\n",
                "# Create cross-tabulation\n",
                "crosstab = pd.crosstab(\n",
                "    correlation_data['high_confidence'],\n",
                "    correlation_data['high_frustration'],\n",
                "    margins=True,\n",
                "    margins_name='Total'\n",
                ")\n",
                "\n",
                "# Rename index and columns for clarity\n",
                "crosstab.index = ['Low/Neutral Confidence', 'High Confidence', 'Total']\n",
                "crosstab.columns = ['Low/Neutral Frustration', 'High Frustration', 'Total']\n",
                "\n",
                "print(\"Cross-Tabulation of Confidence and Frustration:\")\n",
                "print(crosstab)\n",
                "print()\n",
                "\n",
                "# Calculate key statistics\n",
                "total_n = len(correlation_data)\n",
                "high_conf_high_frust = ((correlation_data['high_confidence']) & (correlation_data['high_frustration'])).sum()\n",
                "high_conf_total = correlation_data['high_confidence'].sum()\n",
                "high_frust_total = correlation_data['high_frustration'].sum()\n",
                "\n",
                "print(f\"Key Findings:\")\n",
                "print(f\"  - Total respondents: {total_n}\")\n",
                "print(f\"  - Students with high confidence: {high_conf_total} ({high_conf_total/total_n*100:.1f}%)\")\n",
                "print(f\"  - Students with high frustration: {high_frust_total} ({high_frust_total/total_n*100:.1f}%)\")\n",
                "print(f\"  - Students with BOTH high confidence AND high frustration: {high_conf_high_frust} ({high_conf_high_frust/total_n*100:.1f}%)\")\n",
                "print()\n",
                "print(f\"Among students with high confidence, {high_conf_high_frust}/{high_conf_total} ({high_conf_high_frust/high_conf_total*100:.1f}%) also report high frustration.\")\n",
                "print(f\"This demonstrates that confidence and frustration can coexist, suggesting frustration may be\")\n",
                "print(f\"related to course difficulty rather than lack of self-efficacy or tool usability issues.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8689af02fzo",
            "metadata": {},
            "source": [
                "##### Relationship with Tool Usability\n",
                "\n",
                "To determine whether frustration is related to the tool or the course content, we examine the relationship between frustration and perceived tool usability. If frustration is independent of usability ratings, this suggests frustration stems from course difficulty rather than tool-related issues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3oifi45bct6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation between Frustration and Usability\n",
                "frust_usability_corr = pg.corr(\n",
                "    correlation_data['Frustration'],\n",
                "    correlation_data['Usability'],\n",
                "    method='spearman'\n",
                ")\n",
                "\n",
                "print(\"Spearman Correlation between Frustration and Usability:\")\n",
                "print(frust_usability_corr)\n",
                "print()\n",
                "\n",
                "# Correlation between Frustration and Transparency\n",
                "frust_transparency_corr = pg.corr(\n",
                "    correlation_data['Frustration'],\n",
                "    correlation_data['Transparency'],\n",
                "    method='spearman'\n",
                ")\n",
                "\n",
                "print(\"Spearman Correlation between Frustration and Transparency:\")\n",
                "print(frust_transparency_corr)\n",
                "print()\n",
                "\n",
                "# Summary interpretation\n",
                "print(\"Summary:\")\n",
                "print(f\"  - Frustration × Usability: ρ = {frust_usability_corr['r'].values[0]:.3f}, p = {frust_usability_corr['p-val'].values[0]:.4f}\")\n",
                "print(f\"  - Frustration × Transparency: ρ = {frust_transparency_corr['r'].values[0]:.3f}, p = {frust_transparency_corr['p-val'].values[0]:.4f}\")\n",
                "print()\n",
                "\n",
                "if (frust_usability_corr['p-val'].values[0] > 0.05) and (frust_transparency_corr['p-val'].values[0] > 0.05):\n",
                "    print(\"Interpretation: Frustration shows no significant correlation with tool usability or transparency.\")\n",
                "    print(\"This indicates that frustration is likely related to course difficulty rather than tool-related issues,\")\n",
                "    print(\"supporting the conclusion that Atlas provides usable, transparent support even in a challenging learning environment.\")\n",
                "elif abs(frust_usability_corr['r'].values[0]) < 0.3 and abs(frust_transparency_corr['r'].values[0]) < 0.3:\n",
                "    print(\"Interpretation: Frustration shows weak or no meaningful correlation with tool characteristics.\")\n",
                "    print(\"This suggests frustration is primarily driven by course content difficulty rather than the tool itself.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lkvajfe9mp",
            "metadata": {},
            "source": [
                "##### Interaction Group Comparison\n",
                "\n",
                "Finally, we examine whether the relationship between confidence and frustration varies across interaction groups (High, Low, No). This helps understand whether different levels of engagement with Atlas relate to different patterns of confidence and frustration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "zz3ptniq4gj",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge interaction group into correlation data\n",
                "correlation_with_group = correlation_data.merge(\n",
                "    post_survey_df[['user_id', 'interaction_group']].drop_duplicates(),\n",
                "    on='user_id',\n",
                "    how='left'\n",
                ")\n",
                "\n",
                "# Descriptive statistics by interaction group\n",
                "print(\"Mean Confidence and Frustration by Interaction Group:\")\n",
                "print(\"=\" * 70)\n",
                "group_stats = correlation_with_group.groupby('interaction_group')[['Confidence', 'Frustration']].agg(['mean', 'std', 'count'])\n",
                "print(group_stats.round(2))\n",
                "print()\n",
                "\n",
                "# Test if there are significant differences in Confidence across groups\n",
                "if correlation_with_group['interaction_group'].notna().any():\n",
                "    kruskal_confidence = pg.kruskal(\n",
                "        data=correlation_with_group.dropna(subset=['interaction_group']),\n",
                "        dv='Confidence',\n",
                "        between='interaction_group'\n",
                "    )\n",
                "    \n",
                "    kruskal_frustration = pg.kruskal(\n",
                "        data=correlation_with_group.dropna(subset=['interaction_group']),\n",
                "        dv='Frustration',\n",
                "        between='interaction_group'\n",
                "    )\n",
                "    \n",
                "    print(\"Kruskal-Wallis Test Results:\")\n",
                "    print(\"-\" * 70)\n",
                "    print(f\"Confidence across interaction groups: H = {kruskal_confidence['H'].values[0]:.3f}, p = {kruskal_confidence['p-unc'].values[0]:.4f}\")\n",
                "    print(f\"Frustration across interaction groups: H = {kruskal_frustration['H'].values[0]:.3f}, p = {kruskal_frustration['p-unc'].values[0]:.4f}\")\n",
                "    print()\n",
                "    \n",
                "    if kruskal_confidence['p-unc'].values[0] > 0.05 and kruskal_frustration['p-unc'].values[0] > 0.05:\n",
                "        print(\"Interpretation: No significant differences in confidence or frustration across interaction groups.\")\n",
                "        print(\"This suggests that the confidence-frustration pattern is consistent regardless of\")\n",
                "        print(\"engagement level with Atlas, further supporting that frustration relates to course\")\n",
                "        print(\"difficulty rather than tool engagement patterns.\")\n",
                "    else:\n",
                "        print(\"Interpretation: Some differences exist across interaction groups.\")\n",
                "        print(\"Further post-hoc analysis would be needed to identify specific group differences.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5bfeeeac7c7ffe48",
            "metadata": {},
            "source": [
                "## Inferential Analysis\n",
                "This section contains the hypothesis tests and functions to interpret the results in natural language."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a3b04ec63e2d064d",
            "metadata": {},
            "source": [
                "### Functions for Hypothesis Testing\n",
                "Functions that are useful for multiple hypothesis tests are defined once here."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "def403ed8e62a6f3",
            "metadata": {},
            "source": [
                "#### Functions for Interpretation of Statistical Results in Natural Language\n",
                "For better readability of the results, we provide functions that turn statistical test results into plain-English summaries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5d59b0317f0685a7",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:04.470843Z",
                    "start_time": "2025-09-27T16:59:04.465015Z"
                }
            },
            "outputs": [],
            "source": [
                "def interpret_kruskal_result(kruskal_df, alpha=0.05):\n",
                "    \"\"\"\n",
                "    Generate a plain-English summary of a Kruskal–Wallis test result.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    kruskal_df : pd.DataFrame\n",
                "        DataFrame with the test output (e.g., from pingouin.kruskal), expected to\n",
                "        contain at least the columns: 'H' (test statistic), 'p-unc' (p-value),\n",
                "        and 'ddof1' (degrees of freedom, typically k-1 groups).\n",
                "        Only the first row is used.\n",
                "    alpha : float, default 0.05\n",
                "        Significance threshold for deciding whether the effect is statistically significant.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    str\n",
                "        A human-readable interpretation including H, df, p, and a significance statement.\n",
                "    \"\"\"\n",
                "    # Extract key statistics from the first row\n",
                "    H = kruskal_df.iloc[0]['H']\n",
                "    p = kruskal_df.iloc[0]['p-unc']\n",
                "    df = int(kruskal_df.iloc[0]['ddof1'])\n",
                "\n",
                "    # Build a formatted report string\n",
                "    interpretation = f\"Kruskal-Wallis test:\\n\"\n",
                "    interpretation += f\"  - H = {H:.3f}\\n\"\n",
                "    interpretation += f\"  - df = {df}\\n\"\n",
                "    interpretation += f\"  - p = {p:.4f}\\n\\n\"\n",
                "\n",
                "    # Decision based on alpha\n",
                "    if p < alpha:\n",
                "        interpretation += \"There is a significant difference between at least two groups.\"\n",
                "    else:\n",
                "        interpretation += \"There is no significant difference between the groups.\"\n",
                "\n",
                "    return interpretation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e49ef11810dcdd05",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:04.761002Z",
                    "start_time": "2025-09-27T16:59:04.755848Z"
                }
            },
            "outputs": [],
            "source": [
                "def interpret_dunn_posthoc_results(posthoc_df, alpha=0.05):\n",
                "    \"\"\"\n",
                "    Generate a plain-English summary of Dunn post hoc pairwise comparisons.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    posthoc_df : pd.DataFrame\n",
                "        Output from `pingouin.pairwise_tests` (or similar) after a Kruskal–Wallis test,\n",
                "        expected to contain at least:\n",
                "          - 'A' and 'B' : names/labels of the two groups being compared\n",
                "          - 'p-corr'    : multiplicity-adjusted p-value (e.g., Bonferroni, Holm)\n",
                "    alpha : float, default 0.05\n",
                "        Significance threshold used to flag comparisons as significant.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    str\n",
                "        A multi-line string summarizing each pairwise comparison with its\n",
                "        adjusted p-value and a significance marker.\n",
                "    \"\"\"\n",
                "    lines = [\"Interpretation of Dunn Posthoc Test:\\n\"]\n",
                "\n",
                "    # Iterate over each pairwise comparison and format its result\n",
                "    for _, row in posthoc_df.iterrows():\n",
                "        group_a = row['A']\n",
                "        group_b = row['B']\n",
                "        p_corr = row['p-corr']  # adjusted p-value for multiple comparisons\n",
                "\n",
                "        # Mark comparison as significant if p < alpha\n",
                "        sig = \"Significant\" if p_corr < alpha else \"Not significant\"\n",
                "\n",
                "        lines.append(f\"- {group_a} vs {group_b}: p = {p_corr:.4f} → {sig}\")\n",
                "\n",
                "    return \"\\n\".join(lines)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "baa212d4cb98fd1e",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:04.897460Z",
                    "start_time": "2025-09-27T16:59:04.882415Z"
                }
            },
            "outputs": [],
            "source": [
                "def interpret_pairwise_results(results: pd.DataFrame, alpha: float = 0.05, use_holm: bool = True) -> str:\n",
                "    \"\"\"\n",
                "    Turn a pairwise comparison table into a plain-language summary.\n",
                "\n",
                "    Expected columns in `results`\n",
                "    -----------------------------\n",
                "    - 'contrast' : string labels like 'high>no', 'high>low', 'low>no', or 'A vs B'\n",
                "    - 'p1', 'p2' : pass rates as proportions (e.g., 0.42)\n",
                "    - 'RR'       : risk ratio\n",
                "    - 'OR'       : odds ratio\n",
                "    - 'p_unadj'  : unadjusted p-value\n",
                "    - 'p_holm'   : Holm-adjusted p-value\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    results : pd.DataFrame\n",
                "        Pairwise comparison metrics as above.\n",
                "    alpha : float, default 0.05\n",
                "        Significance threshold.\n",
                "    use_holm : bool, default True\n",
                "        If True, decisions use Holm-adjusted p-values; otherwise unadjusted p.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    str\n",
                "        A multi-line string with bullet-point interpretations and a short takeaway.\n",
                "    \"\"\"\n",
                "    # Helpers: percent formatting, compact p-value, rounding\n",
                "    def pp(x):   # format proportion as percentage with 1 decimal\n",
                "        return f\"{x*100:.1f}%\"\n",
                "    def pval(x):  # compact p-value, e.g., p=.0284\n",
                "        return f\"p={x:.4f}\".replace(\"0.\", \".\")\n",
                "    def rnd(x, d=2):\n",
                "        return f\"{x:.{d}f}\"\n",
                "\n",
                "    # Choose which p-value to use and its label\n",
                "    pcol = 'p_holm' if use_holm else 'p_unadj'\n",
                "    label_p = \"Holm-adjusted p\" if use_holm else \"p\"\n",
                "\n",
                "    lines = []\n",
                "    sig_flags = []\n",
                "\n",
                "    for _, row in results.iterrows():\n",
                "        contrast = str(row['contrast'])\n",
                "\n",
                "        # Parse group labels and the comparison direction from 'contrast'\n",
                "        if '>' in contrast:\n",
                "            g1, g2 = contrast.split('>')\n",
                "            direction = \"higher pass rate than\"\n",
                "        elif 'vs' in contrast:\n",
                "            g1, g2 = [s.strip() for s in contrast.split('vs')]\n",
                "            direction = \"different pass rate than\"\n",
                "        else:\n",
                "            # Fallback parsing if a custom separator is used\n",
                "            parts = contrast.replace(' ', '').split(',')\n",
                "            g1, g2 = parts[0], parts[1] if len(parts) > 1 else (\"Group1\", \"Group2\")\n",
                "            direction = \"higher pass rate than\"\n",
                "\n",
                "        # Extract metrics for this pair\n",
                "        p1 = float(row['p1']); p2 = float(row['p2'])\n",
                "        rd = p1 - p2                          # risk difference\n",
                "        rr = float(row['RR'])                 # risk ratio\n",
                "        or_ = float(row['OR'])                # odds ratio\n",
                "        p_used = float(row[pcol])             # chosen p-value\n",
                "\n",
                "        # Significance decision\n",
                "        sig = p_used < alpha\n",
                "        sig_flags.append(sig)\n",
                "\n",
                "        verdict = \"Significant\" if sig else \"Not significant\"\n",
                "        arrow = \"↑\" if rd > 0 else (\"↓\" if rd < 0 else \"→\")\n",
                "\n",
                "        # Build the line with metrics and a brief interpretation\n",
                "        lines.append(\n",
                "            f\"- **{g1} vs {g2}**: {arrow} {pp(p1)} vs {pp(p2)} \"\n",
                "            f\"(Δ={pp(rd)}, RR={rnd(rr,2)}, OR={rnd(or_,2)}, {label_p} {pval(p_used)}) — {verdict}.\"\n",
                "            + (f\" Interpretation: **{g1}** has a {pp(rd)} higher value than **{g2}**.\"\n",
                "               if sig and rd > 0 else\n",
                "               f\" Interpretation: no reliable evidence that **{g1}** has a higher value than **{g2}**.\")\n",
                "        )\n",
                "\n",
                "    # Overall takeaway summarizing how many contrasts are significant\n",
                "    n_sig = sum(sig_flags)\n",
                "    if n_sig == 0:\n",
                "        takeaway = f\"No pairwise difference is significant at α={alpha:.2f}\" + (\" (Holm-adjusted).\" if use_holm else \".\")\n",
                "    elif n_sig == 1:\n",
                "        idx = sig_flags.index(True)\n",
                "        cn = results.iloc[idx]['contrast']\n",
                "        takeaway = f\"Only **{cn}** is significant at α={alpha:.2f}\" + (\" (Holm-adjusted).\" if use_holm else \".\")\n",
                "    else:\n",
                "        sig_list = \", \".join(f\"**{results.iloc[i]['contrast']}**\" for i, s in enumerate(sig_flags) if s)\n",
                "        takeaway = f\"Significant differences at α={alpha:.2f}: {sig_list}\" + (\" (Holm-adjusted).\" if use_holm else \".\")\n",
                "\n",
                "    header = f\"Pairwise interpretation ({label_p}, α={alpha:.2f}):\"\n",
                "    return \"\\n\".join([header, *lines, \"\", f\"**Takeaway:** {takeaway}\"])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34b2b8e27fc44521",
            "metadata": {},
            "source": [
                "#### Statistical Test for pairwise comparison of portions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "811e9b530cc45bd2",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:04.990038Z",
                    "start_time": "2025-09-27T16:59:04.978928Z"
                }
            },
            "outputs": [],
            "source": [
                "def test_percentage_pairs(data: pd.DataFrame, column, one_sided: bool = True) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Compare proportions (share of True) between interaction groups using z-tests,\n",
                "    and report effect sizes (RR, OR) with Holm-adjusted p-values.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    data : pd.DataFrame\n",
                "        Must contain 'interaction_group' and the target `column`.\n",
                "    column : str\n",
                "        Column with boolean condition (or values convertible to bool) indicating \"success\".\n",
                "    one_sided : bool, default True\n",
                "        If True, tests H1: p_g1 > p_g2 (alternative='larger'); otherwise two-sided.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    pd.DataFrame\n",
                "        Rows for the pairs High>Low, High>No, Low>No with:\n",
                "        ['contrast','p1','p2','RR','OR','z','p_unadj','p_holm'] (rounded to 4 d.p.).\n",
                "    \"\"\"\n",
                "    # Cross-tab: rows = interaction_group, columns = boolean outcome (False/True)\n",
                "    # Reindex to fixed order and fill missing with 0\n",
                "    ct = (pd.crosstab(data['interaction_group'], data[column].astype(bool))\n",
                "            .reindex(['No','Low','High'], fill_value=0))\n",
                "\n",
                "    # Define pairwise comparisons (order matters for one-sided tests)\n",
                "    pairs = [('High','Low'), ('High','No'), ('Low','No')]\n",
                "    alt   = 'larger' if one_sided else 'two-sided'\n",
                "\n",
                "    rows = []\n",
                "    for g1, g2 in pairs:\n",
                "        # Group totals and number of successes (True)\n",
                "        n1, n2 = ct.loc[g1].sum(), ct.loc[g2].sum()\n",
                "        x1, x2 = int(ct.loc[g1, True]), int(ct.loc[g2, True])\n",
                "\n",
                "        # Two-proportion z-test (statsmodels.stats.proportion.proportions_ztest)\n",
                "        z, p = proportions_ztest([x1, x2], [n1, n2], alternative=alt)\n",
                "\n",
                "        # 2x2 table for effect sizes (statsmodels.stats.contingency_tables.Table2x2)\n",
                "        t = Table2x2(np.array([[x1, n1 - x1],\n",
                "                               [x2, n2 - x2]], float))\n",
                "\n",
                "        rows.append({\n",
                "            'contrast': f'{g1}>{g2}' if one_sided else f'{g1} vs {g2}',\n",
                "            'p1': x1 / n1,\n",
                "            'p2': x2 / n2,\n",
                "            'RR': t.riskratio,\n",
                "            'OR': t.oddsratio,\n",
                "            'z': z,\n",
                "            'p_unadj': float(p)\n",
                "        })\n",
                "\n",
                "    out = pd.DataFrame(rows)\n",
                "\n",
                "    # Family-wise error control across the three tests (Holm step-down)\n",
                "    out['p_holm'] = multipletests(out['p_unadj'], method='holm')[1]\n",
                "\n",
                "    return out[['contrast', 'p1', 'p2', 'RR', 'OR', 'z', 'p_unadj', 'p_holm']].round(4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f02df956dad8511",
            "metadata": {},
            "source": [
                "### Mastery Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "54e374ced81caa30",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.100536Z",
                    "start_time": "2025-09-27T16:59:05.070078Z"
                }
            },
            "outputs": [],
            "source": [
                "# Add mastery score and mastery progress to competency_user_df\n",
                "mastery_df = competency_user_df.merge(\n",
                "    competency_df[[\"id\", \"mastery_threshold\"]],\n",
                "    left_on=\"competency_id\",\n",
                "    right_on=\"id\",\n",
                "    how=\"left\"\n",
                ")\n",
                "\n",
                "# Calculate mastery score\n",
                "mastery_df[\"mastery\"] = (mastery_df[\"progress\"] * mastery_df[\"confidence\"]).clip(lower=0, upper=100)\n",
                "\n",
                "# Add score to the data frame\n",
                "mastery_df[\"mastery_progress\"] = (mastery_df[\"mastery\"] / mastery_df[\"mastery_threshold\"]).clip(lower=0, upper=1)\n",
                "\n",
                "# Only consider the first time that users reach their maximum mastery progress\n",
                "# Search for index of maximum mastery progress per user\n",
                "idx = full_df.groupby('user_id')['mastery_progress'].idxmax()\n",
                "\n",
                "# Filter the DataFrame to only include these rows\n",
                "max_mastery_per_user = full_df.loc[idx].sort_values('user_id').reset_index(drop=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ab9ccb3a5c78537",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.173330Z",
                    "start_time": "2025-09-27T16:59:05.162450Z"
                }
            },
            "outputs": [],
            "source": [
                "# Kruskal-Wallis-Test\n",
                "kruskal = pg.kruskal(\n",
                "    data=max_mastery_per_user,\n",
                "    dv='mastery_progress',\n",
                "    between='interaction_group'\n",
                ")\n",
                "\n",
                "print(\"\\nKruskal-Wallis Result:\")\n",
                "print(kruskal)\n",
                "print(interpret_kruskal_result(kruskal))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bb7553d4dc1c78f9",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.244177Z",
                    "start_time": "2025-09-27T16:59:05.231916Z"
                }
            },
            "outputs": [],
            "source": [
                "# Non-parametric Posthoc — Dunn's Test\n",
                "posthoc = pg.pairwise_tests(\n",
                "    data=max_mastery_per_user,\n",
                "    dv='mastery_progress',\n",
                "    between='interaction_group',\n",
                "    parametric=False,\n",
                "    padjust='bonferroni'\n",
                ")\n",
                "\n",
                "print(\"\\nDunn Posthoc Test Result:\")\n",
                "print(posthoc)\n",
                "print(interpret_dunn_posthoc_results(posthoc))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92598c2193eff3f0",
            "metadata": {},
            "source": [
                "### Exam Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4824ee243ba9d920",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.292896Z",
                    "start_time": "2025-09-27T16:59:05.288357Z"
                }
            },
            "outputs": [],
            "source": [
                "# Kruskal-Wallis-Test\n",
                "kruskal = pg.kruskal(\n",
                "    data=users,\n",
                "    dv='total_exam_score',\n",
                "    between='interaction_group'\n",
                ")\n",
                "print(\"\\nKruskal-Wallis Result:\")\n",
                "print(kruskal)\n",
                "print(interpret_kruskal_result(kruskal))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "959fa3827f2d60f2",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.348911Z",
                    "start_time": "2025-09-27T16:59:05.339268Z"
                }
            },
            "outputs": [],
            "source": [
                "# Non-parametric Posthoc — Dunn's Test\n",
                "posthoc = pg.pairwise_tests(\n",
                "    data=users,\n",
                "    dv='total_exam_score',\n",
                "    between='interaction_group',\n",
                "    parametric=False,\n",
                "    padjust='bonferroni'\n",
                ")\n",
                "\n",
                "print(\"\\nDunn Posthoc Test Result:\")\n",
                "print(posthoc)\n",
                "print(interpret_dunn_posthoc_results(posthoc))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c775c11a1f3365a",
            "metadata": {},
            "source": [
                "### Passing Rate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eaa3e80445a2d532",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.535540Z",
                    "start_time": "2025-09-27T16:59:05.353423Z"
                }
            },
            "outputs": [],
            "source": [
                "# Test for statistical significance of differences in passing rates\n",
                "result = test_percentage_pairs(users, 'passed', one_sided=True)\n",
                "print(result)\n",
                "print(interpret_pairwise_results(result))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a2e3b912bfee6148",
            "metadata": {},
            "source": [
                "### Practice Mode"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1802a34c5fb96d0d",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.712084Z",
                    "start_time": "2025-09-27T16:59:05.618082Z"
                }
            },
            "outputs": [],
            "source": [
                "# Test for statistical significance of differences in handed in test runs\n",
                "result = test_percentage_pairs(participation_df, 'test_run', one_sided=True)\n",
                "print(result)\n",
                "print(interpret_pairwise_results(result))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96c8eb94760b4d3e",
            "metadata": {},
            "source": [
                "### Learning Path Started"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ddd556e66b8955c",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.805850Z",
                    "start_time": "2025-09-27T16:59:05.714329Z"
                }
            },
            "outputs": [],
            "source": [
                "# Test for statistical significance of differences in the percentage of users who started the learning path\n",
                "result = test_percentage_pairs(learning_path_df, 'started_by_student', one_sided=True)\n",
                "print(result)\n",
                "print(interpret_pairwise_results(result))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "68427bdaebe2985a",
            "metadata": {},
            "source": [
                "### Learning Path Interactions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cc2ae2ce07f1e32",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:05.874835Z",
                    "start_time": "2025-09-27T16:59:05.851713Z"
                }
            },
            "outputs": [],
            "source": [
                "# Kruskal-Wallis-Test\n",
                "kruskal = pg.kruskal(\n",
                "    data=lp_interactions_per_user,\n",
                "    dv='interactions',\n",
                "    between='interaction_group'\n",
                ")\n",
                "\n",
                "print(\"\\nKruskal-Wallis Result:\")\n",
                "print(kruskal)\n",
                "print(interpret_kruskal_result(kruskal))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "383bd6fcc1fa7446",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:06.017026Z",
                    "start_time": "2025-09-27T16:59:06.005943Z"
                }
            },
            "outputs": [],
            "source": [
                "# Non-parametric Posthoc — Dunn's Test (not Tukey)\n",
                "posthoc = pg.pairwise_tests(\n",
                "    data=lp_interactions_per_user,\n",
                "    dv='interactions',\n",
                "    between='interaction_group',\n",
                "    parametric=False,\n",
                "    padjust='bonferroni'\n",
                ")\n",
                "\n",
                "print(\"\\nDunn Posthoc Test Result:\")\n",
                "print(posthoc)\n",
                "print(interpret_dunn_posthoc_results(posthoc))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a967555eafd5396d",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2025-09-27T16:59:06.067175Z",
                    "start_time": "2025-09-27T16:59:06.065841Z"
                }
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 2
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython2",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}